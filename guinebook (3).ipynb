{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n\ndatapath = \"/kaggle/input/vinbigdata-chest-xray-abnormalities-detection/\"\nmetapath = \"/kaggle/input/xraydata/xraydata\"\ndata = pd.read_csv(f\"{datapath}/train.csv\")\ndata = data.set_index(\"image_id\")\ndata[\"width\"] = (data[\"x_max\"]-data[\"x_min\"])\ndata[\"height\"] = (data[\"y_max\"]-data[\"y_min\"])\ndata[\"area\"] = (data[\"height\"]*data[\"width\"])\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Test and Train data creation\nTrain and Test data is created by\n\n1. Combining all 67K annotations into 15K, one for each image\n2. Splitting 15K into approx 12/3K train/test ensuring distribution of classes across gender is maintained in both.\n3. expanded train/test by creating 320x320 images of full, 2x2 and 4x4 sections of xray images while sampling normal xray/sections with a probability of 0.2 only. This gave us close to 80K train and 20K test images. \n4. for each dataset we also stoe metadate that help us to create corresponding masks for training."},{"metadata":{"trusted":true},"cell_type":"code","source":"trainmeta = pd.read_csv(f\"{metapath}/traindata.csv\").set_index(\"id\")\ntrainmeta.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testmeta = pd.read_csv(f\"{metapath}/testdata.csv\").set_index(\"id\")\ntestmeta.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NUM_CLASSES = 14","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dataset Creation"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport torch\nfrom torch.utils.data import Dataset\nimport random\nimport re\nimport albumentations as A\nfrom torchvision import transforms\nimport albumentations.pytorch as AP\nimport skimage\nimport cv2\nfrom PIL import Image\nfrom skimage.transform import resize\n\n\nclass AlbumentationTransforms:\n    \"\"\"\n    Helper class to create test and train transforms using Albumentations\n    \"\"\"\n    def __init__(self, transforms_list=[]):\n        transforms_list.append(AP.ToTensor())\n        self.transforms = A.Compose(transforms_list)\n        \n    def __call__(self, img):\n        img = np.array(img)\n        #print(img)\n        return self.transforms(image=img)['image']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from skimage import io\n\nclass DataLoader:\n    \"\"\"\n    Helper class to load test and train data\n    \"\"\"\n    def __init__(self, shuffle=True, batch_size=128, seed=1):\n        cuda = torch.cuda.is_available()\n\n        if cuda:\n            torch.cuda.manual_seed(seed)\n\n        # dataloader arguments - something you'll fetch these from cmdprmt\n        self.dataloader_args = dict(shuffle=shuffle, batch_size=batch_size, num_workers=4, \n                                    pin_memory=True) if cuda else dict(shuffle=shuffle, \n                                                                       batch_size=batch_size)\n\n    def load(self, data):\n        return torch.utils.data.DataLoader(data, **self.dataloader_args)\n\n\n      \nclass XrayDataset(Dataset):\n    \"\"\"hest XRay dataset reader.\"\"\"\n\n    def __init__(self, filepath, data, levels, size, channel_mean, channel_stdev, meta, transforms = None):\n        \"\"\"\n        Args:\n            data (string):\n        \"\"\"\n        self.transforms = transforms\n        self.filepath = filepath\n        self.images = data\n        self.size = size # must macth the image size\n        self.means = channel_mean\n        self.stdevs = channel_stdev\n        self.meta = meta\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n            \n        image = io.imread(os.path.join(self.filepath, f\"{self.images[idx]:08}.png\"), as_gray=True, pilmode=\"L\")\n        # get the meta\n        info = self.meta.loc[self.images[idx]]\n        wr = (info.resized_width/info.original_width)/info.dim_ratio\n        hr = (info.resized_height/info.original_height)/info.dim_ratio\n        #if idx==3:\n        #    print(idx, info)\n        # create masks from info\n        #me = metadata.loc[image]\n        annots = data.loc[info['image_id']]\n        labels = [info['c0'], info['c1'], info['c2'], info['c3'], info['c4'], info['c5'], info['c6'], info['c7'], \n                  info['c8'], info['c9'], info['c10'], info['c11'], info['c12'], info['c13']]\n        masks = np.zeros(shape=[NUM_CLASSES, self.size, self.size], dtype=\"float\")\n        if not info['c14']:\n            for cid in range(13):\n                if labels[cid]:\n                    class_instances = annots[annots[\"class_id\"]==cid]\n                    for index, instance in class_instances.iterrows():\n                        \n                        # get the left, bottom, top and right extents of the instance\n                        # in the new size. But we have not take into account our view\n                        l = max(int(instance.x_min), int(info['left'])) - int(info['left'])\n                        r = min(int(instance.x_max), int(info['right'])) - int(info['left'])\n                        b = max(int(instance.y_min), int(info['top'])) - int(info['top'])\n                        t = min(int(instance.y_max), int(info['bottom'])) - int(info['top'])\n                        \n                        if l<r and b<t:\n                            #if idx==3:\n                            #    print(instance, \"\\n\", \"----------\", \"\\n\", cid, l, r, b, t, \"\\n\", \"----------\")\n                            l = round(l*wr)\n                            r = round(r*wr)\n                            b = round(b*hr)\n                            t = round(t*hr)\n                            \n                            masks[cid, b:t, l:r] = 1  \n        # convert set labels to a list\n        labels = np.array(labels)       \n\n        if self.transforms:\n            image = self.transforms(image)\n\n        # Bhargav Sir to calculate mean and stdev of the dataset. Uncomment below after that\n        image = (image - self.means)/self.stdevs\n        image = torch.transpose(image, 1, 0)\n        image = torch.reshape(image, (1, image.shape[1], image.shape[0]))\n        #image = torch.from_numpy(image)\n        masks = torch.from_numpy(masks)\n        labels = torch.from_numpy(labels)\n        \n        # Decide if we are sending meta - sex, pixel spacing, positional encoding\n        # calculate that here from meta, window, dim\n\n        return image.float(), masks.float() #, labels.float(), idx","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(trainmeta), len(testmeta))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Setting the data\n\nBelow we can pick a subset of data instead. Entire data will take 7 hours to run\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_data, test_data = trainmeta.index, testmeta.index # for full data\ntrain_data, test_data = trainmeta.index[:10000], testmeta.index[:2500] # for part data say","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data Properties\nIMAGE_WIDTH = 320\nHIERARCHY_LEVELS = 1\nBATCH_SIZE = 16\nchannel_mean = 0.51615639\nchannel_stdev = 0.24926406\n\ntransforms = AlbumentationTransforms(\n                [\n                    #A.Resize(80, 80),\n                    # A.Normalize(mean=channel_mean, std=channel_stdev),\n                    #A.CLAHE(always_apply=True),\n                    A.RandomBrightnessContrast(),\n                    #A.InvertImg(),\n                    A.GaussNoise()\n                ]\n             )\ntest_tranforms = AlbumentationTransforms()\ntrain = XrayDataset(f\"{metapath}/traindata\", train_data, HIERARCHY_LEVELS, IMAGE_WIDTH, \n                    channel_mean, channel_stdev, trainmeta, transforms = transforms)\ntest = XrayDataset(f\"{metapath}/testdata\", test_data, HIERARCHY_LEVELS, IMAGE_WIDTH, \n                   channel_mean, channel_stdev, testmeta, transforms = test_tranforms)\n\ndataloader = DataLoader(batch_size=BATCH_SIZE, shuffle=True)\n\n# train dataloader\ntrain_loader = dataloader.load(train)\n\n# test dataloader\ntest_loader = dataloader.load(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"images, masks = iter(test_loader).next()\nimages.shape\nc = 0\nim = Image.fromarray(((images[c][0].numpy()*channel_stdev + channel_mean)*255).astype('uint8'))\nim\nmi = Image.fromarray((masks[c][0].numpy()*255).astype('uint8'))\nmi"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install torchsummary","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model train/test setup"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Copied from EVA4. Need significat changes. This is just for an idea. \n# Follow Conclusions from https://github.com/abhinavdayal/DepthMask to make changes\n# other than just input and output shape changes\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchsummary import summary\nfrom tqdm.notebook import tqdm\n\nclass Train:\n    def __init__(self, model, dataloader, optimizer, runmanager, lossfn, scheduler=None, L1lambda = 0):\n        self.model = model\n        self.dataloader = dataloader\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n        self.L1lambda = L1lambda\n        self.lossfn = lossfn\n        self.runmanager = runmanager\n\n    def run(self):\n        self.model.train()\n        pbar = tqdm(self.dataloader)\n        # TODO: meta may come as fourth thing here\n        for data, target in pbar:\n            self.runmanager.begin_batch()\n            # get samples\n            data, target = data.to(self.model.device), target.to(self.model.device)\n\n            # Init\n            self.optimizer.zero_grad()\n            # In PyTorch, we need to set the gradients to zero before starting to do backpropragation because PyTorch accumulates the gradients on subsequent backward passes. \n            # Because of this, when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly.\n\n            # Predict\n            y_pred = self.model(data)\n\n            # Calculate loss\n            loss = self.lossfn(y_pred, target)\n\n            #Implementing L1 regularization\n            if self.L1lambda > 0:\n                reg_loss = 0.\n                for param in self.model.parameters():\n                    reg_loss += torch.sum(param.abs())\n                loss += self.L1lambda * reg_loss\n\n\n            # Backpropagation\n            loss.backward()\n            self.optimizer.step()\n\n            # Update pbar-tqdm\n            self.runmanager.track_train_loss(loss)\n            \n            lr = 0\n            if self.scheduler:\n                lr = self.scheduler.get_last_lr()[0]\n            else:\n                # not recalling why i used sekf.optimizer.lr_scheduler.get_last_lr[0]\n                lr = self.optimizer.param_groups[0]['lr']\n\n            batchtime = self.runmanager.end_batch(lr)\n            pbar.set_description(f'time: {batchtime:0.2f}, loss: {loss.item():0.4f}')\n            \n            if self.scheduler:\n                self.scheduler.step()\n\nclass Test:\n    def __init__(self, model, dataloader, runmanager, lossfn, scheduler=None):\n        self.model = model\n        self.dataloader = dataloader\n        self.runmanager = runmanager\n        self.scheduler = scheduler\n        self.lossfn = lossfn\n        print(\"initialized tester with \", self.model.device)\n\n    def run(self):\n        self.model.eval()\n        with torch.no_grad():\n            pbar = tqdm(self.dataloader)\n            for data, target in pbar:\n                data, target = data.to(self.model.device), target.to(self.model.device)\n                output = self.model(data)\n                loss = self.lossfn(output, target)\n                self.runmanager.track_test_loss(loss)\n\n            if self.scheduler and isinstance(self.scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n                pbar.write(\"In scheduler step with loss of \", self.runmanager.get_test_loss())\n                self.scheduler.step(self.runmanager.get_test_loss())\n\nclass ModelTrainer:\n    def __init__(self, model, optimizer, train_loader, test_loader, runmanager, \n                 lossfn, scheduler=None, batch_scheduler=False, L1lambda = 0):\n        self.model = model\n        print(self.model.device)\n        self.model.to(self.model.device)\n        self.scheduler = scheduler\n        self.batch_scheduler = batch_scheduler\n        self.optimizer = optimizer\n        self.runmanager = runmanager\n        self.lossfn = lossfn\n        self.train_loader = train_loader\n        self.test_loader = test_loader\n        self.train = Train(model, train_loader, optimizer, self.runmanager, self.lossfn, \n                           self.scheduler if self.batch_scheduler else None, L1lambda)\n        self.test = Test(model, test_loader, self.runmanager, self.lossfn, self.scheduler)\n\n\n    def run(self, runparams, epochs=10):\n        pbar = tqdm(range(1, epochs+1), desc=\"Epochs\")\n        self.runmanager.begin_run(runparams, self.train_loader, self.test_loader)\n        for epoch in pbar:\n            self.runmanager.begin_epoch()\n            self.train.run()\n            self.test.run()\n            lr = self.optimizer.param_groups[0]['lr']\n            pbar.write(self.runmanager.end_epoch(lr))\n            self.runmanager.savebest(self.model.name)\n            # need to ake it more readable and allow for other schedulers\n            if self.scheduler and not self.batch_scheduler and not isinstance(self.scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n                self.scheduler.step()\n            pbar.write(f\"Learning Rate = {lr:0.6f}\")\n\n        # save stats for later lookup\n        self.runmanager.save(self.model.name)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# A Unet styled Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nclass Net(nn.Module):\n    def __init__(self, name=\"Model\"):\n        super(Net, self).__init__()\n        self.name = name\n        \n    def summary(self, input_size): #input_size=(1, 28, 28)\n        summary(self, input_size=input_size)\n\n    def gotrain(self, optimizer, train_loader, test_loader, epochs, runmanager, runparams, lossfn, \n                scheduler=None, batch_scheduler=False, L1lambda=0):\n        self.trainer = ModelTrainer(self, optimizer, train_loader, test_loader, runmanager, lossfn, \n                                    scheduler, batch_scheduler, L1lambda)\n        self.trainer.run(runparams, epochs)\n\n\nclass InitialBlock(nn.Module):\n    def __init__(self, planes):\n        super(InitialBlock, self).__init__()\n        self.conv1 = nn.Conv2d(1, planes, kernel_size=3, padding=1, stride=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes*2, kernel_size=3, padding=1, stride=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes*2)\n        self.conv3 = nn.Conv2d(planes*2, planes*4, kernel_size=3, padding=1, stride=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes*4)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = F.relu(self.bn2(self.conv2(out)))\n        out = F.relu(self.bn3(self.conv3(out)))\n        return  out\n\nclass EncoderPath(nn.Module):\n    def __init__(self, inplanes, outplanes, dilation):\n        super(EncoderPath, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, outplanes, kernel_size=3, padding=dilation, \n                               stride=2, dilation=dilation, bias=False)\n        self.bn1 = nn.BatchNorm2d(outplanes)\n        self.conv2 = nn.Conv2d(outplanes, outplanes, kernel_size=3, padding=dilation, \n                               stride=1, dilation=dilation, bias=False)\n        self.bn2 = nn.BatchNorm2d(outplanes)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        return out\n\n\nclass EncoderBlock(nn.Module):\n    def __init__(self, inplanes, outplanes):\n        super(EncoderBlock, self).__init__()\n        self.direct = nn.Conv2d(inplanes, outplanes//4, kernel_size=1, padding=0, \n                                stride=2, bias=False)\n        self.directbn = nn.BatchNorm2d(outplanes//4)\n        # if we need to reduce we can do groupwise here with shuffle, worth a try\n        self.path1 = EncoderPath(inplanes, outplanes//4, 1)\n        self.path2 = EncoderPath(inplanes, outplanes//4, 2)\n        self.path3 = EncoderPath(inplanes, outplanes//4, 4)\n\n    def forward(self, x):\n        p1 = self.path1(x)\n        p2 = self.path2(x)\n        p3 = self.path3(x)\n        x = self.directbn(self.direct(x))\n        out = torch.cat((x, p1, p2, p3), 1)\n        out = F.relu(out)\n        return out\n\nclass DecoderBlock(nn.Module):\n    def __init__(self, planes):\n        super(DecoderBlock, self).__init__()\n        #self.upsample = nn.ConvTranspose2d(planes*4, planes*4, kernel_size=3, stride=2, padding=1)\n        # At this point we will use Pixel Shuffle to make resolution 224x224 \n        planes = planes//4 #due to pixel shuffle\n        # it may be useful to shuffle before adding groups?\n        self.conv1 = nn.Conv2d(planes, planes, kernel_size=3, padding=1, stride=1, \n                               bias=False, groups = 1)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1, stride=1, \n                               bias=False, groups = 1)\n        self.bn2 = nn.BatchNorm2d(planes)\n\n    def forward(self, x):\n        out = F.pixel_shuffle(x, 2) # 32 channels\n        out = F.relu(self.bn1(self.conv1(out))) # 64 channels\n        out = self.bn2(self.conv2(out)) # 128 channels\n        return out\n\nclass Encoder(nn.Module):\n    def __init__(self, planes):\n        super(Encoder,self).__init__()\n        self.encoder1 = EncoderBlock(planes, planes*2)   # 128 channels# RF = 24\n        self.encoder2 = EncoderBlock(planes*2, planes*4)  # 256 channels\n        self.encoder3 = EncoderBlock(planes*4, planes*8)  # 512 channels\n\n    def forward(self,x):\n        e1 = self.encoder1(x) # 32 channels 80x80\n        e2 = self.encoder2(e1) # 64 channels 40x40\n        e3 = self.encoder3(e2) # 128 channels 20x20\n\n        return e1, e2, e3\n\nclass MinMaxScaler(nn.Module):\n    def __init(self):\n        super(MinMaxScaler, self).__init__()\n\n    def forward(self, x):\n        s = x.shape\n        y = x.view(s[0], s[1], -1) \n        y = y - y.min(2, keepdim=True)[0]\n        y = y/(y.max(2, keepdim=True)[0] )\n        y = y.view(s)\n        return y\n\n\nclass Decoder(nn.Module):\n    def __init__(self, planes):\n        super(Decoder,self).__init__()\n        self.decoder1 = DecoderBlock(planes)   # 512 INPUT  AND 128 OUTPUT\n        self.decoder2 = DecoderBlock(planes//2)  # 256 Input 64 output\n        self.decoder3 = DecoderBlock(planes//4)  # 128 Input 32 output\n        # e2 has 256 outputs\n        self.e2conv = nn.Conv2d(planes//2, planes//4, kernel_size=1, padding=0, stride=1, bias=False)\n        self.e2bn = nn.BatchNorm2d(planes//4) # 64 channels\n        #e1 has 128 outputs\n        self.e1conv = nn.Conv2d(planes//4, planes//8, kernel_size=1, padding=0, stride=1, bias=False)\n        self.e1bn = nn.BatchNorm2d(planes//8) # 64 channels\n        # e0 has 64 outputs\n        self.e0conv = nn.Conv2d(planes//8, planes//16, kernel_size=1, padding=0, stride=1, bias=False)\n        self.e0bn = nn.BatchNorm2d(planes//16) # 32 channels\n\n        self.conv1 = nn.Conv2d(planes//8, planes//8, kernel_size=3, padding=1, stride=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes//8)\n\n        self.conv2 = nn.Conv2d(planes//8, planes//8, kernel_size=3, padding=1, stride=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes//8)\n\n        self.conv3 = nn.Conv2d(planes//8, 14, kernel_size=1, stride=1, bias=False)\n        self.minmaxscaler = MinMaxScaler()\n\n   \n    def forward(self, *inputs):\n\n        x, e2, e1, e0 = inputs\n        d = self.decoder1(x) # 32 channels 80x80\n        e2 = self.e2bn(self.e2conv(e2))\n        d = F.relu(torch.cat((d, e2), 1))\n\n        d = self.decoder2(d) # 32 channels 80x80\n        e1 = self.e1bn(self.e1conv(e1))\n        d = F.relu(torch.cat((d, e1), 1))\n\n        d = self.decoder3(d) # 64 channels 40x40\n        e0 = self.e0bn(self.e0conv(e0))\n        d = F.relu(torch.cat((d, e0), 1))\n\n        d = F.relu(self.bn1(self.conv1(d)))\n        d = F.relu(self.bn2(self.conv2(d)))\n\n        d = self.conv3(d)\n        return self.minmaxscaler(d)\n\n\n#implementation of the new resnet model\nclass XrayEncoderDecoder(Net):\n    def __init__(self,name=\"XrayEncoderDecoder\", planes=16):\n        super(XrayEncoderDecoder,self).__init__(name)\n        self.prepLayer = InitialBlock(planes)  # 64 channels\n        self.encoder = Encoder(planes*4)  # 512 channels\n        self.decoder = Decoder(planes*32)\n\n    def forward(self,x):\n        data_shape = x.size()\n        e0 = self.prepLayer(x) # 32 channels 160x160\n        e1, e2, e3 = self.encoder(e0) # 32 channels 80x80\n        return self.decoder(e3, e2, e1, e0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Init weights"},{"metadata":{"trusted":true},"cell_type":"code","source":"def init_weights(m):\n    if str(type(m)).startswith('torch.nn.modules.conv') :\n        torch.nn.init.kaiming_normal_(m.weight, a=0, mode='fan_in', nonlinearity='leaky_relu')\n\nmodel = XrayEncoderDecoder(name=\"XrayEncoderDecoder\")\nmodel.apply(init_weights)\n\nuse_cuda = torch.cuda.is_available()\nmodel.device = torch.device(\"cuda\" if use_cuda else \"cpu\")\nmodel.to(model.device)\nmodel.summary((1,384, 384))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Managing stats and backup"},{"metadata":{"trusted":true},"cell_type":"code","source":"# COPIED from EVA4. Need to see if this would work in Kaggle context \n# Need to be cleaned up\n# import standard PyTorch modules\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.tensorboard import SummaryWriter # TensorBoard support\n\n# import torchvision module to handle image manipulation\nimport torchvision\nimport torchvision.transforms as transforms\n\n# calculate train time, writing train data to files etc.\nimport time\nimport pandas as pd\nimport json\nimport os\n\n# import modules to build RunBuilder and RunManager helper classes\nfrom collections  import OrderedDict\nfrom collections import namedtuple\nfrom itertools import product\n\n# Helper class, help track loss, accuracy, epoch time, run time, \n# hyper-parameters etc. Also record to TensorBoard and write into csv, json\nclass RunManager():\n    def __init__(self, savepath, channel_means, channel_stdevs, network):\n\n        # tracking every epoch count, loss, accuracy, time\n        self.epoch_count = 0\n        self.batch_count = 0\n        self.min_val_loss = 10e10\n        self.epoch_train_loss = 0\n        self.epoch_start_time = None\n        self.batch_start_time = None\n        self.savepath = savepath\n        self.network = network\n\n        # tracking every run count, run data, hyper-params used, time\n        self.run_params = None\n        self.run_count = 0\n        self.run_data = []\n        self.run_start_time = None\n\n        # record model, trainloader and TensorBoard \n        self.network = network\n        self.trainloader = None\n        self.channel_means = channel_means\n        self.channel_stdevs = channel_stdevs\n          \n    \n    # record the count, hyper-param, model, trainloader of each run\n    # record sample images and network graph to TensorBoard  \n    def begin_run(self, run, trainloader, testloader):\n        self.run_start_time = time.time()\n\n        self.run_params = run\n        self.run_count += 1\n\n        self.trainloader = trainloader\n        self.testloader = testloader\n        self.batchlrs = []\n\n        import socket\n        from datetime import datetime\n        current_time = datetime.now().strftime('%b%d_%H-%M-%S')\n        log_dir = os.path.join(self.savepath,f'runs/{current_time}_{socket.gethostname()}-{run}')\n\n    # when run ends, close TensorBoard, zero epoch count\n    def end_run(self):\n        self.epoch_count = 0\n        self.batch_count = 0\n\n    def begin_batch(self):\n        self.batch_start_time = time.time()\n\n    def end_batch(self, lr):\n        self.batch_count += 1\n        self.batchlrs.append((lr, self.batch_count))\n        batch_duration = time.time() - self.batch_start_time\n        return batch_duration\n\n    # zero epoch count, loss, accuracy, \n    def begin_epoch(self):\n        self.epoch_start_time = time.time()\n        self.epoch_count += 1\n        self.epoch_train_loss = 0\n        self.epoch_test_loss = 0\n        self.batchlrs = []\n        self.batchloss = []\n\n    def end_epoch(self, lr):\n        # calculate epoch duration and run duration(accumulate)\n        epoch_duration = time.time() - self.epoch_start_time\n        run_duration = time.time() - self.run_start_time\n\n        # record epoch loss and accuracy\n        trainloss = self.get_train_loss()\n        testloss = self.get_test_loss()\n        \n\n        results = OrderedDict()\n        results[\"run\"] = self.run_count\n        results[\"epoch\"] = self.epoch_count\n        results[\"train loss\"] = trainloss\n        results[\"test loss\"] = testloss\n        # Write into 'results' (OrderedDict) for all run related data\n        results[\"epoch duration\"] = epoch_duration\n        results[\"run duration\"] = run_duration   \n\n        # Record hyper-params into 'results'\n        for k,v in self.run_params.items(): \n            results[k] = v\n        self.run_data.append(results)\n\n        return f'{results}'\n\n    # accumulate loss of batch into entire epoch loss\n    def track_train_loss(self, loss):\n        # multiply batch size so variety of batch sizes can be compared\n        self.batchloss.append((loss.item(), self.batch_count+1))\n        self.epoch_train_loss += loss.item() * self.trainloader.batch_size\n\n    def track_test_loss(self, loss):\n        self.epoch_test_loss += loss.item() * self.testloader.batch_size\n        \n    def get_test_loss(self):\n        return self.epoch_test_loss / len(self.testloader.dataset)\n\n    def get_train_loss(self):\n        return self.epoch_train_loss / len(self.trainloader.dataset)\n\n    def savebest(self, fileName):\n        f = os.path.join(self.savepath, f'{fileName}.pt')\n        if self.epoch_test_loss < self.min_val_loss:\n            torch.save(self.network.state_dict(), f)\n\n    # save end results of all runs into csv, json for further analysis\n    def save(self, fileName):\n        f = os.path.join(self.savepath, f'{fileName}.csv')\n        pd.DataFrame.from_dict(\n            self.run_data, \n            orient = 'columns',\n        ).to_csv(f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\noutdir = '/kaggle/working/output'\nif not os.path.exists(outdir):\n    os.mkdir(outdir)\n\n#check if the model.name file exists\nsavedmodel = os.path.join(outdir, model.name+'.pt')\n\nif os.path.exists(savedmodel):\n    model.load_state_dict(torch.load(savedmodel))\n\n\"\"\"\nclassification=False, \n\"\"\"\n\"\"\"\nclasses={'Aortic enlargement':1, 'Atelectasis':2, 'Calcification': 3,\n          'Cardiomegaly': 4, 'Consolidation': 5, 'ILD': 6, 'Infiltration':7,\n          'Lung Opacity': 8, 'Nodule/Mass': 9, 'Other lesion': 10, \n          'Pleural effusion': 11, 'Pleural thickening': 12, 'Pneumothorax': 13,\n          'Pulmonary fibrosis': 14\n         }\n\"\"\"\n\nm = RunManager(outdir, channel_mean, channel_stdev, model)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LOSS\n\nWe need to try MSE Loss only separately. "},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nfrom math import exp\nimport numpy as np\nimport torch.nn as nn\n\nclass WeightedLoss(nn.Module):\n    def __init__(self, lossfn, weights):\n        super(WeightedLoss, self).__init__()\n        self.lossfn = lossfn\n        self.weights = weights\n\n    def forward(self, source, target):\n        \"\"\"\n        We get one batch\n        In this batch some images will be normal and some may have pathology\n        We need to apply weight for each class\n        But each image in batch will have multiple classes or be normal\n        \n        \"\"\"\n        loss = torch.tensor(0).float().to(target.device)\n        normal = torch.sum(target) == 0\n        #print(source.shape)\n        for j in range(source.shape[0]): # for each image j in he batch\n            for i in range(len(self.weights)-1): # for each class i in the image\n                #print(i, j, source[j:j+1,i:i+1,:,:].shape, target[j:j+1,i:i+1,:,:].shape)\n                l = self.lossfn(source[j:j+1,i:i+1,:,:], target[j:j+1,i:i+1,:,:])\n                w = self.weights[-1] if normal else self.weights[i]\n                loss += w*l\n                #print(w, l, loss)\n        \n        return loss\n\nclass MixedLoss(nn.Module):\n    def __init__(self, loss1, loss2, alpha):\n        super(MixedLoss, self).__init__()\n        self.loss1 = loss1\n        self.loss2 = loss2\n        self.alpha = alpha\n\n    def forward(self, source, target):\n        loss1 = self.loss1(source, target)\n        loss2 = self.loss2(source, target)\n        #print(\"loss1 = \", loss1, \"loss2 = \", loss2)\n        loss = self.alpha*loss1 + (1 - self.alpha)*loss2\n        return loss\n\n# below code is adapted from\n# https://github.com/jorge-pessoa/pytorch-msssim/blob/master/pytorch_msssim/__init__.py\n# only change is to make MSSSIM as a loss or difference measure instead of similarity measure\n\ndef gaussian(window_size, sigma):\n    gauss = torch.Tensor([exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])\n    return gauss/gauss.sum()\n\n\ndef create_window(window_size, channel=1):\n    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n    window = _2D_window.expand(channel, 1, window_size, window_size).contiguous()\n    return window\n\n\ndef ssim(img1, img2, window_size=11, window=None, size_average=True, full=False, val_range=None):\n    # Value range can be different from 255. Other common ranges are 1 (sigmoid) and 2 (tanh).\n    if val_range is None:\n        if torch.max(img1) > 128:\n            max_val = 255\n        else:\n            max_val = 1\n\n        if torch.min(img1) < -0.5:\n            min_val = -1\n        else:\n            min_val = 0\n        L = max_val - min_val\n    else:\n        L = val_range\n\n    padd = 0\n    (_, channel, height, width) = img1.size()\n    if window is None:\n        real_size = min(window_size, height, width)\n        window = create_window(real_size, channel=channel).to(img1.device)\n\n    mu1 = F.conv2d(img1, window, padding=padd, groups=channel)\n    mu2 = F.conv2d(img2, window, padding=padd, groups=channel)\n\n    mu1_sq = mu1.pow(2)\n    mu2_sq = mu2.pow(2)\n    mu1_mu2 = mu1 * mu2\n\n    sigma1_sq = F.conv2d(img1 * img1, window, padding=padd, groups=channel) - mu1_sq\n    sigma2_sq = F.conv2d(img2 * img2, window, padding=padd, groups=channel) - mu2_sq\n    sigma12 = F.conv2d(img1 * img2, window, padding=padd, groups=channel) - mu1_mu2\n\n    C1 = (0.01 * L) ** 2\n    C2 = (0.03 * L) ** 2\n\n    v1 = 2.0 * sigma12 + C2\n    v2 = sigma1_sq + sigma2_sq + C2\n    cs = torch.mean(v1 / v2)  # contrast sensitivity\n\n    ssim_map = ((2 * mu1_mu2 + C1) * v1) / ((mu1_sq + mu2_sq + C1) * v2)\n\n    if size_average:\n        ret = ssim_map.mean()\n    else:\n        ret = ssim_map.mean(1).mean(1).mean(1)\n\n    if full:\n        return ret, cs\n    return ret\n\n\ndef msssim(img1, img2, window_size=11, size_average=True, val_range=None, normalize=True):\n    device = img1.device\n    weights = torch.FloatTensor([0.0448, 0.2856, 0.3001, 0.2363, 0.1333]).to(device)\n    levels = weights.size()[0]\n    mssim = []\n    mcs = []\n    for _ in range(levels):\n        sim, cs = ssim(img1, img2, window_size=window_size, size_average=size_average, full=True, val_range=val_range)\n        mssim.append(sim)\n        mcs.append(cs)\n\n        img1 = F.avg_pool2d(img1, (2, 2))\n        img2 = F.avg_pool2d(img2, (2, 2))\n\n    mssim = torch.stack(mssim)\n    mcs = torch.stack(mcs)\n\n    # Normalize (to avoid NaNs during training unstable models, not compliant with original definition)\n    if normalize:\n        mssim = (mssim + 1) / 2\n        mcs = (mcs + 1) / 2\n\n    pow1 = mcs ** weights\n    pow2 = mssim ** weights\n    # From Matlab implementation https://ece.uwaterloo.ca/~z70wang/research/iwssim/\n    output = torch.prod(pow1[:-1] * pow2[-1])\n    return output\n\n\nclass MSSSIM(torch.nn.Module):\n    def __init__(self, window_size=11, size_average=True, channel=3):\n        super(MSSSIM, self).__init__()\n        self.window_size = window_size\n        self.size_average = size_average\n        self.channel = channel\n\n    def forward(self, img1, img2):\n        # TODO: store window between calls if possible\n        s = msssim(img1, img2, window_size=self.window_size, size_average=self.size_average)\n        # we need loss\n        loss = torch.clamp((1 - s)*0.5, min=0, max=1)\n\n        if self.size_average == 'mean':\n            loss = torch.mean(loss)\n\n        return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"L1lambda = 0\nL2lambda = 1e-4\n# TODO: We have 14 classes  that are imbalanced\n# We need to find the imbalance in dataset and for each of the 14\n# do a weighting. \nlossfn = MixedLoss(nn.L1Loss(), MSSSIM(21), 0.16)\nimbalance_weights = (1-torch.tensor([3067, 186, 452, 2300, 353, 38, 613, 1322, 826, 1134, 1032, 1981, 96, 1617, 10606])/15000).float()\nimbalance_weights = imbalance_weights.to(device = (\"cuda\" if use_cuda else \"cpu\"))\n\n# we can replace the weighted loss with MSE loss plainly and see.\ncriterion = WeightedLoss(lossfn, imbalance_weights) # nn.MSELoss()\n\noptimizer = optim.SGD(model.parameters(), lr=1e-2) #optim.Adam(model.parameters(), lr=1e-5, weight_decay=L2lambda) # \n#lr_finder = LRRangeFinder(model, optimizer, criterion, 1, 1e-2, 100, train_loader)\n#lr_finder.range_test()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.optim as optim\nimport torch.nn as nn\nimport torch\n#https://research.nvidia.com/sites/default/files/pubs/2017-03_Loss-Functions-for/NN_ImgProc.pdf\nL1lambda = 0\nL2lambda = 0\nEPOCHS = 4\n\nmax_lr = 0.3\noptimizer = optim.SGD(model.parameters(), lr=max_lr/100, momentum=0.9, nesterov=True, weight_decay=L2lambda)\n# scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=max_lr, steps_per_epoch=len(train_loader), div_factor=5, pct_start=0.2, epochs=EPOCHS)\n# scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=max_lr, steps_per_epoch=int(len(train)/batch_size)+1, epochs=EPOCHS,  pct_start=5/24, div_factor=600, final_div_factor=1 )\n#scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=max_lr, steps_per_epoch=int(len(train_loader))+1, epochs=EPOCHS,  pct_start=0.2, div_factor=10, final_div_factor=10 )\nscheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr = 0.003, max_lr=max_lr, mode='triangular2')\n\nm = RunManager(outdir, channel_mean, channel_stdev, model)\n\nmodel.gotrain(optimizer, train_loader, test_loader, EPOCHS, m, {'lr':'cyclic', 'loss':'MSSSIM+L1'}, \n              criterion, scheduler, True, L1lambda)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"model.eval()\n\nwith torch.no_grad():\n    images, target = iter(test_loader).next()\n    images, target = images.to(model.device), target.to(model.device)\n    output = model(images)"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"for i in range(output.shape[0]):\n    print(torch.sum(target[i]), torch.sum(output[i]))"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"import matplotlib.pyplot as plt\nc = 3\nim = Image.fromarray(((images[c][0].cpu().numpy()*channel_stdev + channel_mean)*255).astype('uint8'))\nfor x in range(14):\n    plt.imshow(output[c][x].cpu().numpy())\n    plt.show()\nim"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"torch.cuda.empty_cache()"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}